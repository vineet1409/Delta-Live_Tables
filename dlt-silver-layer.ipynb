{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f71ba13-a902-4fc2-b318-743593ac9a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "! pip install dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fcd029-5cb4-4ec8-b13b-4a88a9f24924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\nCollecting termcolor>=1.1.0\n  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\nCollecting flatbuffers>=23.1.21\n  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (21.3)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\nCollecting tensorflow-estimator<2.14,>=2.13.0\n  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting tensorboard<2.14,>=2.13\n  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\nCollecting absl-py>=1.0.0\n  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\nCollecting wrapt>=1.11.0\n  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\nCollecting keras<2.14,>=2.13.1\n  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting numpy<=1.24.3,>=1.22\n  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nCollecting h5py>=2.9.0\n  Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\nCollecting google-auth-oauthlib<1.1,>=0.5\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting markdown>=2.6.8\n  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\nCollecting werkzeug>=1.0.1\n  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\nCollecting google-auth<3,>=1.6.3\n  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.16.0-py3-none-any.whl (6.7 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\nCollecting oauthlib>=3.0.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting MarkupSafe>=2.1.1\n  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, werkzeug, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.3 which is incompatible.\nSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0 zipp-3.16.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow=='2.11.0'\n",
    "! pip install numpy=='1.21.5'\n",
    "! pip install transformers=='4.29.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b20065-1b6e-4fb6-995b-e0f67ea682c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vineet1409/fine-tuned-AlBERT were not used when initializing TFAlbertForSequenceClassification: ['dropout_9']\n- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at vineet1409/fine-tuned-AlBERT.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\nSome layers from the model checkpoint at vineet1409/fine-tuned-bioclinical-BERT were not used when initializing TFBertForSequenceClassification: ['dropout_112']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertForSequenceClassification were initialized from the model checkpoint at vineet1409/fine-tuned-bioclinical-BERT.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned ALBERT model\n",
    "model_lib = \"vineet1409/fine-tuned-AlBERT\"  \n",
    "albert_classifier = pipeline(\"text-classification\", model=model_lib, tokenizer=model_lib)\n",
    "\n",
    "# Load the bio-clinical-BERT model and tokenizer\n",
    "clinicalbert_model = TFBertForSequenceClassification.from_pretrained(\"vineet1409/fine-tuned-bioclinical-BERT\")\n",
    "clinicalbert_tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53d21f6-9134-4b20-97be-26c14e58888b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## silver Layer albert\n",
    "import logging\n",
    "from pyspark.sql.functions import array, lit, when, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_analysis(text):\n",
    "    predictions = albert_classifier(text)  \n",
    "    predicted_label = predictions[0]['label']\n",
    "    if predicted_label == 'LABEL_1':\n",
    "        result = 'Suicidal'\n",
    "    elif predicted_label == 'LABEL_0':\n",
    "        result = 'Non-suicidal'\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "@dlt.table(\n",
    "comment=\"Silver_Layer_albert: Custom LLM {ALBERT} Predictions on dataset.\"\n",
    ")\n",
    "def albert_silver_layer():\n",
    "    logger.info(\"Reading source table\")  \n",
    "    df =  dlt.read('bronze_layer')\n",
    "    df = df.limit(20)\n",
    "\n",
    "    text_col = col(\"text\")\n",
    "    text_list = df.select(text_col).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Perform analysis and store the results in a list\n",
    "    results = []\n",
    "    for text in text_list:\n",
    "        result = run_analysis(text)\n",
    "        results.append(result)\n",
    "\n",
    "    results = [r for r in results if r is not None] \n",
    "    df = df.withColumn(\"albert-predictions\", array(*[lit(r) for r in results]))\n",
    "    df = df.withColumn(\"albert-predictions\", array(*[when(col(r).isNull(), lit(\"\")).otherwise(lit(r)) for r in results]))\n",
    "    if results:\n",
    "        df = df.withColumn(\"albert-predictions\", F.concat_ws(\",\", results))\n",
    "    else:\n",
    "        df = df.withColumn(\"albert-predictions\", F.lit(None))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a235c983-4258-42e4-84b4-9769d6180652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Layer: clinical-bert  \n",
    "import logging\n",
    "from pyspark.sql.functions import array, lit, when, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "comment=\"Silver_Layer_clinicalbert: Custom LLM {clinicalbert} Predictions on dataset.\"\n",
    ")\n",
    "def clinicalbert_silver_layer():  \n",
    "    df = dlt.read('bronze_layer')\n",
    "    df = df.limit(20)\n",
    "\n",
    "    text_col = col(\"text\")\n",
    "    text_list = df.select(text_col).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for text in text_list:\n",
    "        encoded_input = clinicalbert_tokenizer.batch_encode_plus(\n",
    "                        [text],\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"tf\",\n",
    "                        max_length=128\n",
    "                        ) \n",
    "        predict = clinicalbert_model.predict(encoded_input['input_ids'])\n",
    "        predicted_labels = np.argmax(predict.logits, axis=1)\n",
    "        class_names = [\"Non-suicidal\", \"Suicidal\"]\n",
    "        predicted_classes = [class_names[label] for label in predicted_labels] \n",
    "        output = predicted_classes[0]\n",
    "        results.append(output)\n",
    "\n",
    "        results = [r for r in results if r is not None] \n",
    "        df = df.withColumn(\"clinical-bert-predictions\", array(*[lit(r) for r in results]))\n",
    "        df = df.withColumn(\"clinical-bert-predictions\", array(*[when(col(r).isNull(), lit(\"\")).otherwise(lit(r)) for r in results]))\n",
    "        if results:\n",
    "            df = df.withColumn(\"clinical-bert-predictions\", F.concat_ws(\",\", results))\n",
    "        else:\n",
    "            df = df.withColumn(\"clinical-bert-predictions\", F.lit(None))\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt-silver-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a1791b-26a8-4714-a7c3-a17ce4c23bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: '1.21.5'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fcd029-5cb4-4ec8-b13b-4a88a9f24924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\nCollecting termcolor>=1.1.0\n  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\nCollecting flatbuffers>=23.1.21\n  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (21.3)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\nCollecting tensorflow-estimator<2.14,>=2.13.0\n  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting tensorboard<2.14,>=2.13\n  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\nCollecting absl-py>=1.0.0\n  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\nCollecting wrapt>=1.11.0\n  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\nCollecting keras<2.14,>=2.13.1\n  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting numpy<=1.24.3,>=1.22\n  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nCollecting h5py>=2.9.0\n  Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\nCollecting google-auth-oauthlib<1.1,>=0.5\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting markdown>=2.6.8\n  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\nCollecting werkzeug>=1.0.1\n  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\nCollecting google-auth<3,>=1.6.3\n  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.16.0-py3-none-any.whl (6.7 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\nCollecting oauthlib>=3.0.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting MarkupSafe>=2.1.1\n  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, werkzeug, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.3 which is incompatible.\nSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0 zipp-3.16.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83572aff-0c03-463c-938e-aa9a4f8eef11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505fe61b-2a91-4af4-b4e9-3b1d423c4079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\r\nCollecting huggingface-hub<1.0,>=0.14.1\r\n  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\r\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from transformers) (21.3)\r\nCollecting pyyaml>=5.1\r\n  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\r\nCollecting safetensors>=0.3.1\r\n  Using cached safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\r\nRequirement already satisfied: numpy>=1.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/lib/python3.9/site-packages (from transformers) (1.24.3)\r\nCollecting regex!=2019.12.17\r\n  Using cached regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\r\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\r\n  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\nCollecting tqdm>=4.27\r\n  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\r\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.1.1)\r\nCollecting fsspec\r\n  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\nInstalling collected packages: tqdm, pyyaml, fsspec, tokenizers, safetensors, regex, huggingface-hub, transformers\r\nSuccessfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 pyyaml-6.0 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting dlt\r\n  Using cached dlt-0.3.3-py3-none-any.whl (373 kB)\r\nCollecting orjson<4.0.0,>=3.8.6\r\n  Using cached orjson-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\r\nCollecting pendulum>=2.1.2\r\n  Using cached pendulum-2.1.2-cp39-cp39-manylinux1_x86_64.whl (155 kB)\r\nCollecting pipdeptree>=2.9.3\r\n  Using cached pipdeptree-2.9.5-py3-none-any.whl (19 kB)\r\nRequirement already satisfied: click>=7.1 in /databricks/python3/lib/python3.9/site-packages (from dlt) (8.0.4)\r\nCollecting cron-descriptor>=1.2.32\r\n  Using cached cron_descriptor-1.4.0-py3-none-any.whl\r\nCollecting tomlkit>=0.11.3\r\n  Using cached tomlkit-0.11.8-py3-none-any.whl (35 kB)\r\nCollecting makefun>=1.15.0\r\n  Using cached makefun-1.15.1-py2.py3-none-any.whl (22 kB)\r\nRequirement already satisfied: astunparse>=1.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/lib/python3.9/site-packages (from dlt) (1.6.3)\r\nCollecting SQLAlchemy>=1.4.0\r\n  Using cached SQLAlchemy-2.0.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\r\nCollecting requirements-parser>=0.5.0\r\n  Using cached requirements_parser-0.5.0-py3-none-any.whl (18 kB)\r\nRequirement already satisfied: fsspec<2024.0.0,>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/lib/python3.9/site-packages (from dlt) (2023.6.0)\r\nCollecting jsonpath-ng<2.0.0,>=1.5.3\r\n  Using cached jsonpath_ng-1.5.3-py3-none-any.whl (29 kB)\r\nCollecting humanize>=4.4.0\r\n  Using cached humanize-4.7.0-py3-none-any.whl (113 kB)\r\nRequirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.9/site-packages (from dlt) (2.27.1)\r\nCollecting gitpython>=3.1.29\r\n  Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\r\nCollecting pathvalidate>=2.5.2\r\n  Using cached pathvalidate-3.0.0-py3-none-any.whl (21 kB)\r\nCollecting pytz>=2022.6\r\n  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\nRequirement already satisfied: PyYAML>=5.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/lib/python3.9/site-packages (from dlt) (6.0)\r\nCollecting hexbytes>=0.2.2\r\n  Using cached hexbytes-0.3.1-py3-none-any.whl (5.9 kB)\r\nCollecting sentry-sdk>=1.4.3\r\n  Using cached sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\r\nCollecting setuptools>=65.6.0\r\n  Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\r\nCollecting deprecated<2.0.0,>=1.2.13\r\n  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\r\nCollecting semver>=2.13.0\r\n  Using cached semver-3.0.1-py3-none-any.whl (17 kB)\r\nCollecting tenacity<9.0.0,>=8.2.2\r\n  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\r\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from dlt) (4.1.1)\r\nCollecting giturlparse>=0.10.0\r\n  Using cached giturlparse-0.10.0-py2.py3-none-any.whl (14 kB)\r\nCollecting simplejson>=3.17.5\r\n  Using cached simplejson-3.19.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\r\nCollecting tzdata>=2022.1\r\n  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\r\nCollecting json-logging==1.4.1rc0\r\n  Using cached json_logging-1.4.1rc0-py2.py3-none-any.whl (22 kB)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.3->dlt) (0.37.0)\r\nRequirement already satisfied: six<2.0,>=1.6.1 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.3->dlt) (1.16.0)\r\nRequirement already satisfied: wrapt<2,>=1.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/lib/python3.9/site-packages (from deprecated<2.0.0,>=1.2.13->dlt) (1.15.0)\r\nCollecting gitdb<5,>=4.0.1\r\n  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\r\nCollecting smmap<6,>=3.0.1\r\n  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\r\nCollecting ply\r\n  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\r\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.9/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->dlt) (5.1.1)\r\nCollecting pytzdata>=2020.1\r\n  Using cached pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\r\nRequirement already satisfied: python-dateutil<3.0,>=2.6 in /databricks/python3/lib/python3.9/site-packages (from pendulum>=2.1.2->dlt) (2.8.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.26.0->dlt) (3.3)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.26.0->dlt) (2.0.4)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.26.0->dlt) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.26.0->dlt) (2021.10.8)\r\nCollecting types-setuptools>=57.0.0\r\n  Using cached types_setuptools-68.0.0.1-py3-none-any.whl (47 kB)\r\nCollecting urllib3<1.27,>=1.21.1\r\n  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\r\nCollecting typing-extensions>=4.0.0\r\n  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\r\nInstalling collected packages: smmap, urllib3, typing-extensions, types-setuptools, pytzdata, ply, greenlet, gitdb, tzdata, tomlkit, tenacity, SQLAlchemy, simplejson, setuptools, sentry-sdk, semver, requirements-parser, pytz, pipdeptree, pendulum, pathvalidate, orjson, makefun, jsonpath-ng, json-logging, humanize, hexbytes, giturlparse, gitpython, deprecated, cron-descriptor, dlt\r\n  Attempting uninstall: urllib3\r\n    Found existing installation: urllib3 1.26.9\r\n    Not uninstalling urllib3 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\r\n    Can't uninstall 'urllib3'. No files were found to uninstall.\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 4.1.1\r\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\r\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n  Attempting uninstall: tenacity\r\n    Found existing installation: tenacity 8.0.1\r\n    Not uninstalling tenacity at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\r\n    Can't uninstall 'tenacity'. No files were found to uninstall.\r\n  Attempting uninstall: setuptools\r\n    Found existing installation: setuptools 61.2.0\r\n    Not uninstalling setuptools at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\r\n    Can't uninstall 'setuptools'. No files were found to uninstall.\r\n  Attempting uninstall: pytz\r\n    Found existing installation: pytz 2021.3\r\n    Not uninstalling pytz at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415\r\n    Can't uninstall 'pytz'. No files were found to uninstall.\r\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001B[0m\r\nSuccessfully installed SQLAlchemy-2.0.18 cron-descriptor-1.4.0 deprecated-1.2.14 dlt-0.3.3 gitdb-4.0.10 gitpython-3.1.32 giturlparse-0.10.0 greenlet-2.0.2 hexbytes-0.3.1 humanize-4.7.0 json-logging-1.4.1rc0 jsonpath-ng-1.5.3 makefun-1.15.1 orjson-3.9.2 pathvalidate-3.0.0 pendulum-2.1.2 pipdeptree-2.9.5 ply-3.11 pytz-2023.3 pytzdata-2020.1 requirements-parser-0.5.0 semver-3.0.1 sentry-sdk-1.28.0 setuptools-68.0.0 simplejson-3.19.1 smmap-5.0.0 tenacity-8.2.2 tomlkit-0.11.8 types-setuptools-68.0.0.1 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-1efa2fcd-8a32-4af4-afa3-ecac78e42415/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b20065-1b6e-4fb6-995b-e0f67ea682c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vineet1409/fine-tuned-AlBERT were not used when initializing TFAlbertForSequenceClassification: ['dropout_9']\n- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at vineet1409/fine-tuned-AlBERT.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\nSome layers from the model checkpoint at vineet1409/fine-tuned-bioclinical-BERT were not used when initializing TFBertForSequenceClassification: ['dropout_112']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertForSequenceClassification were initialized from the model checkpoint at vineet1409/fine-tuned-bioclinical-BERT.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned ALBERT model\n",
    "model_lib = \"vineet1409/fine-tuned-AlBERT\"  \n",
    "albert_classifier = pipeline(\"text-classification\", model=model_lib, tokenizer=model_lib)\n",
    "\n",
    "# Load the bio-clinical-BERT model and tokenizer\n",
    "clinicalbert_model = TFBertForSequenceClassification.from_pretrained(\"vineet1409/fine-tuned-bioclinical-BERT\")\n",
    "clinicalbert_tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa7e12a-2374-4c6b-82ce-dd7bafc7b00a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Layer- ALBERT\n",
    "\n",
    "import dlt\n",
    "\n",
    "def run_analysis(text):\n",
    "    try:\n",
    "        predictions = albert_classifier(text)  # Assuming albert_classifier is defined elsewhere\n",
    "        predicted_label = predictions[0]['label']\n",
    "        if predicted_label == 'LABEL_1':\n",
    "            result = 'Suicidal'\n",
    "        elif predicted_label == 'LABEL_0':\n",
    "            result = 'Non-suicidal'\n",
    "        return result\n",
    "    except Exception:\n",
    "        result = ' '\n",
    "        return result\n",
    "    \n",
    "try:\n",
    "    @dlt.table(\n",
    "    comment=\"Silver_Layer_albert: Custom LLM {ALBERT} Predictions on dataset.\"\n",
    "    )\n",
    "    def mental_health_dlt_albert_silver():  \n",
    "        df = dlt.read('mental_health_dlt_bronze')\n",
    "        pandas_df = df.toPandas()\n",
    "        pandas_df = pandas_df.iloc[:20,] # taking only 20 records as of now\n",
    "        texts = pandas_df['text'].tolist()\n",
    "\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = run_analysis(text)\n",
    "            results.append(result)\n",
    "\n",
    "        pandas_df['albert-predictions'] = results\n",
    "        spark_df_silver_albert = spark.createDataFrame(pandas_df)\n",
    "\n",
    "        return spark_df_silver_albert\n",
    "\n",
    "except AttributeError:\n",
    "    class dlt:  \n",
    "        def table(comment, **options): # Mock the @dlt.table attribute so that it is seen as syntactically valid below  \n",
    "            def _(f):  \n",
    "                pass  \n",
    "            return _; \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b95d962-6304-4a68-b0b8-b1267b91f530",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Silver Layer clinicalbert\n",
    "import dlt \n",
    "  \n",
    "try:\n",
    "    @dlt.table(\n",
    "    comment=\"Silver_Layer_clinicalbert: Custom LLM {clinicalbert} Predictions on dataset.\"\n",
    "    )\n",
    "    def mental_health_dlt_clinicalbert_silver():  \n",
    "        df = dlt.read('mental_health_dlt_bronze')\n",
    "        pandas_df_cb = df.toPandas()\n",
    "        pandas_df_cb = pandas_df_cb.iloc[:20,]\n",
    "        texts = pandas_df_cb['text'].tolist()\n",
    "\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            encoded_input = clinicalbert_tokenizer.batch_encode_plus(\n",
    "                            [text],\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"tf\",\n",
    "                            max_length=128\n",
    "                            ) \n",
    "            predict = clinicalbert_model.predict(encoded_input['input_ids'])\n",
    "            predicted_labels = np.argmax(predict.logits, axis=1)\n",
    "            class_names = [\"Non-suicidal\", \"Suicidal\"]\n",
    "            predicted_classes = [class_names[label] for label in predicted_labels] \n",
    "            output = predicted_classes[0]\n",
    "            results.append(output)\n",
    "\n",
    "        pandas_df_cb['clinicalbert-predictions'] = results\n",
    "        spark_df_silver_clinicalbert = spark.createDataFrame(pandas_df_cb)\n",
    "        \n",
    "        return spark_df_silver_clinicalbert\n",
    "\n",
    "\n",
    "except AttributeError:\n",
    "    class dlt:  \n",
    "        def table(comment, **options): # Mock the @dlt.table attribute so that it is seen as syntactically valid below  \n",
    "            def _(f):  \n",
    "                pass  \n",
    "            return _; \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a235c983-4258-42e4-84b4-9769d6180652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt-silver-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
